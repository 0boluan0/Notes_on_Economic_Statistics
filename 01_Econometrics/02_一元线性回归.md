45# 回归：从身高问题的分析讲起、
## 起源

> 回归：regression
* 源自生物统计中的概念。最初被用于描述子代与父代的身高关系，子代身高与父代身高有较强的关联。
* ![[Pasted image 20240228202716.png]]
* 每一组的均值都是条件均值。即：在父代的身高确定的情况下，子代身高样本的均值。表示为![[Pasted image 20240228131447.png]]
* y的实际水平会随着其他的一系列的不可控的随机因素的干扰，故给原式中加入一个随机扰动（误差），以符合现实情况。

## 意义
**y的取值最终会向由解释变量x所决定的平均水平上回复，这种回复的趋势叫做回归（regression）。**
>注意：一定是y向x回归。且不是向x的值回归，而是向着x决定的那个均值回归。

---

# 一元线性回归（书上第三章）

## 总体回归方程

![[Pasted image 20240228132651.png]]
 **反应最真实的数量关系**，是真实存在的对象，是研究者想要研究的，但是事实上无法真实全知的值。
 * 总体回归方程将我们要研究的数据分成了两个部分，一个是前边两个部分，这部分我们可以理解并解释，而后面的扰动项我们无法解释
 * 方程的来源：前沿的研究与朴素的共性认知。 
N是样本容量，根据容量不同分为[[零散知识点#大样本与小样本|大样本和小样本]]。
![[Pasted image 20240305082342.png]]
条件期望（不是均值，总体只有期望没有均值，反之样本只有均值没有期望）
### 推导：如何从[[Pasted image 20240228132651.png|总体回归方程]]推导到[[Pasted image 20240305082342.png|总体期望方程]]

对总体回归方程左右同取期望，u的期望是0，得到了总体的期望方程
### 为什么u的期望一定是0

深层原因是因为贝塔0的存在，任何不为0的值都可以归到贝塔0中去。

## 样本回归方程

![[Pasted image 20240304203122.png]]
其中的贝塔0hat和贝塔1hat依靠[[零散知识点#残差|残差]]决定。
已知![[Pasted image 20240305130551.png]]
那么残差的值就是取Y和Yhat这两个列向量的[[零散知识点#欧几里得距离|欧几里得距离]]，即取残差平方和。

# 最小二乘法ordinary least square

二乘就是平方的意思，引进的时候翻译不到位。

## 推导

由上面的过程得知，现在要求残差平方和最小，即![[Pasted image 20240305131708.png]]

要求其极小值，只需要一阶偏导得零即可，不需要二阶偏导为正，因为现在要求的是残差平方和极小值，而残差平方和不存在极大值，故其一阶偏导为0时就一定是极小值点。

**由此推知一阶优化条件**：
![[Pasted image 20240305132555.png]]

由一阶优化条件得到正规方程组
## 正规方程组：

![[Pasted image 20240305133741.png]]

>如何理解正规方程组的意义？

事实啥高是对贝塔0hat和贝塔1hat做偏导得到的

### 意义

![[Pasted image 20240306174249.png]]
是对贝塔0hat求偏导得到的，意味着残差的均值为0

![[Pasted image 20240306174325.png]]

是对贝塔1hat求偏导得到的，意味着x和uhat的样本协方差为0，因为这个除以自由度就是样本协方差的公式，这个式子相当于是分子.即说明**xi和ui正交**
所以这两个公式是由第一个推导得到第二个。

### 思考：为什么不用协方差

协方差受量纲影响，所以不是很好。

### 思考：为什么不取绝对值

绝对值函数不连续，取绝对值后没法求导。

### 辨析：欧几里得距离和曼哈顿距离

* [[零散知识点#欧几里得距离|欧几里得距离]]是用两点与坐标系构成的直角三角形的斜边的距离来描述两点的距离
* 曼哈顿距离是用两点与坐标系构成的支教三角形的两直角边的距离相加来描述两点的距离


## 贝塔0hat和贝塔1hat的解法

![[Pasted image 20240310184421.png]]
贝塔1hat有意义且重要，相比之下贝塔0hat没有严格的意义但还是必须得有，没有则正规方程组不成立
在对应具体样本时，贝塔0hat和贝塔1hat是[[零散知识点#估计量 estimator|估计量]]（最小二乘估计量）
但是在不对应具体样本时，二者为[[零散知识点#统计量|统计量]]
# 高斯-马尔可夫定理
## 基本假定（经典假定）

### 1. 线性
回归方程对参数（系数）线性
下面这些都算线性
![[Pasted image 20240310184955.png]]
### 2.解释变量x严格外生
 ![[Pasted image 20240310185423.png]]
注意此处并没有下表的限制，意味着任意的x和任意的u都不相关，即u不受过去现在将来的任何x的影响，即不可被观测，这显然是一种极其苛刻的要求，事实上是不存在这样的情况的。
### 3. 误差项零均值、同方差、无自相关
![[Pasted image 20240310185642.png]]
u零均值就是正则表达式的第一条的情况
同方差就好比俩人去量同一个桌子的高度，两个人的细心程度是一样的，这就意味着我在算桌子的真实高度时就可以进行简单的加算，而最小二乘法用的就是加算，所以必须满足这个要求
无自相关就好比去分别找两个人借100块钱，两人都答应了，但是你不知道这俩人是一家的，事实上有50块是共有的，最后会对你能借到的钱产生影响。

## 高斯-马尔可夫定理内容
![[Pasted image 20240310190106.png]]
最小方差线性无偏估计(Best Linear Unbiased Estimator)
### 1. 线性

![[Pasted image 20240310190123.png]]
换句话说就是y是b的线性组合
正态分布的线性组合还是正态分布假定扰动项正态分布，那么y也会是正态分布，构造已知分布的统计量，命题都是对参数的约束

### 2. 无偏性（有限样本）一致性（大样本）
我们事实上是在用贝塔hat估计贝塔，就像打靶一样，无偏意味着我的弹孔围绕着那个靶心，虽然环数不同，但是最后算出的平均的中心点肯定跟靶子的十环重合。贝塔hat减去贝塔就做误差，而贝塔hat的期望和贝塔的差值叫偏误。
严格外生则无偏，如果x和u正相关，则贝塔hat会被系统性高估。
推导过程见备忘录
严格外生很好，但是极难实现
对于大样本来讲，只需要实现弱外生就行（x和自己对应的u无关就行），这种情况叫做一致性。![[Pasted image 20240313092346.png]]

### 3. 最优性
误差项零均值：前面写过了
同方差：因为OLS直接加总没有赋权，所以在异方差情况下总的方差一定增大，故而要求同方差。
无自相关：保证信息的独立。

## 拟合优度

### 思想
![[Pasted image 20240313092834.png]]
这是对于模型的评价。
y的变化由x的影响与u的影响两部分组成，我们是想让x的部分尽可能大，u的部分尽可能小。

### 推导

1. 方差分解
	即将总的波动划分为解释变量的影响与随机冲击的影响
	![[Pasted image 20240313154426.png]]
	其中，最后一项实际上是没有除自由度的样本协方差
2. 由y与u正交可知，最后一项的数值为0
	![[Pasted image 20240313154546.png]]
3. 此时我们达成了第一步的目的，将总波动划分为了两个部分。
	![[Pasted image 20240313154633.png]]，随机冲击的部分，RSS
	![[Pasted image 20240313154727.png]]，总的部分，TSS
	![[Pasted image 20240313154743.png]]，解释变量的影响部分，ESS，解释平方和。
4. 用![[Pasted image 20240313154820.png]]，来表示解释变量的变化对于总的变化的影响程度，通常来讲是越高越好。

## 参数的约束检验  

假设u的分布为正态分布，